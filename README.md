# Open Science project 2024-02-02
This is an Open Science project about Wikipedia. 
There are 2 ways of accessing our data and codes. One is through the google drive [https://drive.google.com/drive/u/0/folders/0AKbFqUUL3ncfUk9PVA] (1). The other is through github [https://github.com/wubba438/Open_Science_B1] (2).
It is worth noting that , due to the limitation of the size of uploading files, we have not managed to include scrapped data done by the previous group and scrapped data done by our group inside our github repository.
But you can find these two files, "block_log_data_old.csv" and "block_log_data_new.csv" in the Google drive link we provide below.

## 1. Google Drive Link
Please download the folder "Open_Science_B1_Wikipedia" from our shared google drive
The introduction to the structure of the folder is present below. 

Introduction to the Open_Science_B1_Wikipedia folder
The codes are located in 3 files. The first 2 files are mainly inherited from the work of another research group, here I have attached the github link to their project : https://github.com/AF-Cabouat/Wikipedia-Block-log

1 Fetch_Block_Log_data.ipynb It does:
- Data scrapping
- Data cleaning
- Check the distribution of "duration" of bocks

2 Analyze_Blocklog data_original.ipynb does:
- Data loading
- Data cleaning
- Data analysis

3 Admin_Analysis_Blocklog_Data.ipynb does:
- Data loading
- Data cleaning
- Data analysis(mainly about the bot administrators)

The data to use (all data before our analysis) could be found in the "data_original" folder. The folder contains:
1 Blocking policy-related pages creation timeline on English Wikipedia.xlsx
2 Blocking rationales dropdown menu tags creation timeline.xlsx
3 freq_policies_monthly.csv
4 MonthlyEditAndEditorsFrom2004-2017.csv

Other folders
1 Data_results : all outputs we generated with the analyzing notebook
2 Scraped_data: empty folder to store the new user scraped data
3 Scraped_data0: the raw scraped data that we collected

Instruction of steps to take for replicating the figures (.....) using our scraped  data:
- Download the back_up folder from the Google Drive
- Open your python software (Visual Code for us) and open the folder back_up: you will see the data files displayed.
- Install dependencies : make sure you install everything documented in the requirements.txt file (code)
- Run the codes from the folder back_up (see summary appendices for How to replicate specific figures)


## 2. Github Repository

## Requirements

Python 3.10^

## Installation
Clone the repository :

```bash
git clone git@github.com:wubba438/Open_Science_B1.git
cd Open_Science_B1
```

Create and activate a virtual envirnment :

```bash
python3 -m venv .my_venv
source .my_venv/bin/activate
```
or

```bash
python -m venv .my_venv
source .my_venv/bin/activate
```

Install dependencies :
make sure you install everything documented in the requirements.txt file

```bash
python -m pip install -r requirements.txt
```


### Files
The codes are located in 3 files. The first 2 files are mainly inherited from the work of another research group, here I have attached the github link to their project : https://github.com/AF-Cabouat/Wikipedia-Block-log

1 Fetch_Block_Log_data.ipynb It does:
- Data scrapping
- Data cleaning
- Check the distribution of "duration" of bocks

2 Analyze_Blocklog data_original.ipynb does:
- Data loading
- Data cleaning
- Data analysis

3 Admin_Analysis_Blocklog_Data.ipynb does:
- Data loading
- Data cleaning
- Data analysis(mainly about the bot administrators)

The data to use (all data before our analysis) could be found in the "data_original" folder. The folder contains:
1 Blocking policy-related pages creation timeline on English Wikipedia.xlsx
2 Blocking rationales dropdown menu tags creation timeline.xlsx
3 freq_policies_monthly.csv
4 MonthlyEditAndEditorsFrom2004-2017.csv

Other folders
1 Data_results : all outputs we generated with the analyzing notebook
2 Scraped_data: empty folder to store the new user scraped data


Analyse_blocklog data_original.ipynb and Admin_Analysis_Blocklog_Data.ipynb are using the block_log_data_new.csv file (our scraped data). If you are working with your own scraped data, please feel free to change the code according to the name of your new csv file.

The results generated by the data analysis will be stored in the "data_results" folder. We have put the results we got inside but you are more than welcome to replicate it yourself.
